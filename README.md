# GPT-2 Fine-Tuned Mental Health Chatbot

![Hugging Face](https://img.shields.io/badge/Hugging%20Face-Model%20Hosted-orange?logo=huggingface)
![Python](https://img.shields.io/badge/Python-3.8%2B-blue?logo=python)
![Transformers](https://img.shields.io/badge/Huggingface-Transformers-yellow?logo=transformers)
![License](https://img.shields.io/github/license/TheCarBun/GPT2-mental-health)

## ğŸ§  Overview

This repository contains a **fine-tuned GPT-2 model** designed for a **mental health chatbot** that provides supportive and empathetic responses to users expressing distress. The model was trained on conversational data to generate comforting and meaningful replies.

## ğŸ“‚ Repository Structure

```
â”œâ”€â”€ fine_tuned_model/
â”‚   â”œâ”€â”€ config.json
â”‚   â”œâ”€â”€ generation_config.json
â”‚   â”œâ”€â”€ merges.txt
â”‚   â”œâ”€â”€ model.safetensors
â”‚   â”œâ”€â”€ special_tokens_map.json
â”‚   â”œâ”€â”€ tokenizer.json
â”‚   â”œâ”€â”€ tokenizer_config.json
â”‚   â”œâ”€â”€ training_args.bin
â”‚   â”œâ”€â”€ vocab.json
â”œâ”€â”€ intents.json   # Dataset used for training
â”œâ”€â”€ train.csv      # Processed training data
â”œâ”€â”€ test.csv       # Processed test data
â”œâ”€â”€ chatbot_data.txt # Raw conversation data
â”œâ”€â”€ README.md      # Documentation
```

## ğŸš€ Model Details

- **Base Model:** GPT-2 (small)
- **Training Data:** Custom dataset containing mental health conversations
- **Objective:** Generate empathetic responses based on user input
- **Training Framework:** Hugging Face `transformers`
- **Fine-tuned on:** Google Colab (CUDA GPU)

## ğŸ”§ Installation & Setup

### 1ï¸âƒ£ Install Dependencies

```bash
pip install transformers torch datasets huggingface_hub
```

### 2ï¸âƒ£ Load the Fine-Tuned Model

```python
from transformers import pipeline

chat_model = pipeline(
    "text-generation",
    model="TheCarBun/GPT-2-fine-tuned-mental-health",
    tokenizer="TheCarBun/GPT-2-fine-tuned-mental-health",
    truncation=True
)

input_text = "User: I'm feeling very down today."
output = chat_model(input_text, max_length=50)
print(output[0]['generated_text'])
```

## ğŸ›  Training Process

### 1ï¸âƒ£ Preprocessing

- Cleaned conversational data from `intents.json`
- Tokenized text using GPT-2 tokenizer

### 2ï¸âƒ£ Training

```python
from transformers import Trainer, TrainingArguments

training_args = TrainingArguments(
    output_dir="fine_tuned_model",
    evaluation_strategy="epoch",
    save_strategy="epoch",
    num_train_epochs=5,
    per_device_train_batch_size=4,
    per_device_eval_batch_size=4,
    logging_dir="./logs",
    logging_steps=10,
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,
    tokenizer=tokenizer
)
trainer.train()
```

## ğŸ“Š Results & Performance

| Epoch | Training Loss | Validation Loss |
| ----- | ------------- | --------------- |
| 1     | 1.1932        | 1.0248          |
| 2     | 0.7532        | 0.7870          |
| 3     | 0.7520        | 0.6927          |
| 4     | 0.6018        | 0.6579          |
| 5     | 0.5192        | 0.6403          |

## ğŸ¯ Future Improvements

- Deploy as a **REST API**
- Fine-tune with **larger datasets**
- Add **sentiment analysis** for improved response generation

## ğŸ“œ License

This project is licensed under the **Apache 2.0 License**.

## ğŸ™Œ Contributing

Contributions are welcome! Feel free to submit a PR or open an issue.

---

ğŸ“ _Maintained by [TheCarBun](https://github.com/TheCarBun)_
